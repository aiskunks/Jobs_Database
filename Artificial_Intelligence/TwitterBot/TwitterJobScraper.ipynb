{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecb14aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from dataclasses import dataclass, field\n",
    "import dataclasses\n",
    "from typing import List\n",
    "import re\n",
    "import json\n",
    "from datetime import datetime\n",
    "import stanza\n",
    "import emoji\n",
    "import html\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ec6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bearerToken = 'AAAAAAAAAAAAAAAAAAAAAHpjjAEAAAAAn%2BCYkmL02wnLmmY1BEn0%2FhZDcAw%3D1yCPthd5oyDAmQdEGowVHmOWmunSflrAK4nWrxneNwYEZ3VDZT'\n",
    "client = tweepy.Client(bearer_token=bearerToken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "458e967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"(#machinelearning OR #AI OR #BigData OR #DataScience OR #Analytics OR #Python OR #ArtificialIntelligence OR #ML OR #DeepLearning OR #TensorFlow OR #PyTorch OR #rstats) (#hiring OR #recruitment OR #recruiting OR #jobs) lang:en has:links -is:retweet\"\n",
    "if len(query) > 512:\n",
    "    raise \"Query longer than 512 characters\"\n",
    "pages = tweepy.Paginator(client.search_recent_tweets, query=query, expansions=['author_id'], tweet_fields=['context_annotations','created_at','author_id','entities','text', 'public_metrics'], user_fields=['username', 'name','location', 'created_at'],limit=400, max_results=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2be0077b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class User():\n",
    "    id: str = ''\n",
    "    name: str = ''\n",
    "    username: str = ''\n",
    "    dateJoined: str = ''\n",
    "    location: str = ''\n",
    "\n",
    "@dataclass\n",
    "class JobTweetData():\n",
    "    id: str = ''\n",
    "    user: User = None\n",
    "    description: str = ''\n",
    "    datePosted: str = '' \n",
    "    likeCount: int = 0\n",
    "    derivedTags: List[str] = field(default_factory=list)\n",
    "    links: List[str] = field(default_factory=list)\n",
    "    tags: List[str] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ad9a413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emoji_free_text(text):\n",
    "    return emoji.replace_emoji(text, replace='', version=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2228accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_char(text):\n",
    "    string = html.unescape(text)\n",
    "    string = html.unescape(string)\n",
    "    return re.sub('&lt;/?[a-z]+&gt;', '', string)\n",
    "\n",
    "\n",
    "jobDataModels = []\n",
    "for page in pages:\n",
    "    userList = dict()\n",
    "    tweets = page\n",
    "    for user in tweets.includes['users']:\n",
    "        userModel = User()\n",
    "        userModel.id = user['id']\n",
    "        userModel.name = get_emoji_free_text(user['name'])\n",
    "        userModel.username = user['username']\n",
    "        userModel.location = user['location']\n",
    "        userModel.dateJoined = user['created_at'].strftime('%m/%d/%Y')\n",
    "        userList[userModel.id] = userModel\n",
    "    for tweet in tweets.data:\n",
    "        jobTweetModel = JobTweetData()\n",
    "        jobTweetModel.id = tweet['id']\n",
    "        currentUser = userList[tweet['author_id']]\n",
    "        jobTweetModel.user = currentUser\n",
    "        jobTweetModel.datePosted = tweet['created_at'].strftime('%m/%d/%Y')\n",
    "        jobTweetModel.description = remove_special_char(get_emoji_free_text(tweet['text']))\n",
    "        jobTweetModel.likeCount = tweet['public_metrics']['like_count']\n",
    "        urlList = list()\n",
    "        for url in tweet['entities']['urls']:\n",
    "            if re.search('twitter', url['expanded_url']):\n",
    "                continue\n",
    "            urlList.append(url['expanded_url'])\n",
    "        if len(urlList) == 0: \n",
    "            continue\n",
    "        for hashtag in tweet['entities']['hashtags']:\n",
    "            jobTweetModel.tags.append(hashtag['tag'])\n",
    "        jobTweetModel.links = urlList\n",
    "        jobDataModels.append(jobTweetModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482ad12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-12 21:44:13 INFO: Loading these models for language: en (English):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | combined  |\n",
      "| ner       | ontonotes |\n",
      "=========================\n",
      "\n",
      "2022-11-12 21:44:13 INFO: Use device: cpu\n",
      "2022-11-12 21:44:13 INFO: Loading: tokenize\n",
      "2022-11-12 21:44:13 INFO: Loading: ner\n"
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,ner', download_method=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96803ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity: Snowflake\ttype: ORG\n",
      "entity: Snowflake\ttype: ORG\n",
      "entity: Python\ttype: GPE\n",
      "entity: Python\ttype: GPE\n",
      "entity: Python\ttype: GPE\n"
     ]
    }
   ],
   "source": [
    "def remove_hashtags(text):\n",
    "    return re.sub(\"#[A-Za-z0-9_]+\",\"\", text.replace('-', ''))\n",
    "\n",
    "for model in jobDataModels:\n",
    "    processed_data = nlp(remove_hashtags(model.description))\n",
    "    for sent in processed_data.sentences:\n",
    "        for ent in sent.ents:\n",
    "            if(ent.type == 'ORG'):\n",
    "                for tag in ent.text.split('\\n'):\n",
    "                    model.derivedTags.append(tag.strip())\n",
    "            elif(ent.type == 'TITLE'):\n",
    "                model.title = ent.text\n",
    "    model.tags = list(set(model.tags))\n",
    "    model.derivedTags = list(set(model.derivedTags))\n",
    "    print(*[f'entity: {ent.text}\\ttype: {ent.type}' ], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7428557",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonMappedData = dict()\n",
    "jsonJobDictList = []\n",
    "for model in jobDataModels:\n",
    "    jsonJobDictList.append(dataclasses.asdict(model))\n",
    "jsonMappedData['twitterJobData'] = jsonJobDictList\n",
    "\n",
    "with open('twitterData'+datetime.now().strftime('%m_%d_%Y')+'.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(jsonMappedData, file, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jobsVenv",
   "language": "python",
   "name": "jobsvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
